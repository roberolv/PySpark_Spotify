{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29cbe71a-eec4-40cd-9611-25ff03bcb943",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7f3ecc586ccb64ed2683ad4223f5732",
     "grade": false,
     "grade_id": "cell-c303019816ad7b1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Canciones en Spotify entre 1958 y 2019\n",
    "\n",
    "Spotify, en su conjunto, tiene una biblioteca de 30 millones de canciones. No solo contiene estas canciones, sino que también define y documenta los atributos de cada una de estas canciones (el tipo de atributos se aclarará a continuación). Esta es una cantidad increíble de información que requiere una arquitectura de Big Data para ser utilizada correctamente. Y es fundamental contar con esta arquitectura porque existe un inmenso potencial para la monetización de estos datos dentro de la industria de la música.\n",
    "\n",
    "El conjunto de datos final que se debe utilizar en el análisis contiene 277.965 filas y 30 columnas para 24.000 canciones. La clave primaria de los datos es una combinación de WeekID y SongID.\n",
    "\n",
    "* url: la URL de la canción en el sitio de la cartelera.\n",
    "* WeekID: la semana para la que se registra el rendimiento de la canción\n",
    "* Week Position: la posición que ocupó la canción durante esa semana (varía de 1 a 100)\n",
    "* Song: título de la canción\n",
    "* Performer: nombre del intérprete\n",
    "* SongID: identificador de Billboard para la canción (concatenación de columnas anteriores)\n",
    "* Instance: número de veces que se ha producido\n",
    "* Previous Week Position: posición de la semana pasada\n",
    "* Peak Position: la mejor posición que jamás haya ocupado\n",
    "* Weeks on Chart: cuántas semanas ha estado en el top 100\n",
    "* top10_tag: si la posición ocupada estaba dentro del top 10\n",
    "* spotify_genre: género personalizado de Spotify (lista por canción)\n",
    "* spotify_track_id: identificación alfanumérica\n",
    "* spotify_track_preview_url: versión preliminar breve de la URL de la pista\n",
    "* spotify_track_album – Álbum al que pertenece la canción\n",
    "* spotify_track_explicit: registro de etiquetas si la pista contiene lenguaje explícito o no\n",
    "* spotify_track_duration_ms: duración de la canción en milisegundos\n",
    "* spotify_track_popularity: qué tan popular es la pista en Spotify\n",
    "* danceability: métrica que define la probabilidad de que la canción se pueda bailar\n",
    "* energy: Métrica que define qué tan enérgica es la canción\n",
    "* key: tonalidad musical en la que se reproduce la canción\n",
    "* loudness: métrica que define el volumen de la canción\n",
    "* mode: pregrabación estéreo o mono\n",
    "* speechiness: Métrica que define cuánto canto vs instrumento hay en la canción\n",
    "* acousticness: métrica que define el grado en que se utilizan los instrumentos acústicos.\n",
    "* instrumentalness: métrica que define cuánto de la canción utiliza instrumentos\n",
    "* liveness: métrica que define el estado de ánimo de una canción\n",
    "* valence: Métrica que define la positividad de la canción\n",
    "* tempo: Métrica que define el tempo de la canción\n",
    "* time_signature: métrica para etiquetar el tiempo de la canción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff79ada0-f547-43de-a48f-d984c826c9d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nombre completo del alumno:** Roberto Oliva Pastor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e5a23e-22fd-4e3a-b9ef-b5f79a15b18e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf30faf0368c811596d3fd8c37ee37c1",
     "grade": false,
     "grade_id": "cell-81ef968c02ad077e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# INSTRUCCIONES\n",
    "\n",
    "En cada celda debes responder a la pregunta formulada, asegurándote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del cálculo quede guardado exactamente en la variable que venía inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). \n",
    "\n",
    "**No olvides borrar la línea *raise NotImplementedError()* de cada celda cuando hayas completado la solución de esa celda y quieras probarla**.\n",
    "\n",
    "Después de cada celda evaluable verás una celda con código. Ejecútala (no modifiques su código) y te dirá si tu solución es correcta o no. Además de esas pruebas, se realizarán algunas más (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la solución correcta o no. Asegúrate de que, al menos, todas las celdas indican que el código es correcto antes de enviar el notebook terminado.\n",
    "\n",
    "*No olvides todas las sentencias import necesarias. Una celda que no pueda ser ejecutada por faltar import no será evaluada*\n",
    "\n",
    "**Nunca se debe redondear ninguna cantidad si no lo pide explícitamente el enunciado**\n",
    "\n",
    "### Cada solución debe escribirse obligatoriamente en la celda habilitada para ello. Cualquier celda adicional que se haya creado durante el desarrollo deberá ser eliminada.\n",
    "\n",
    "Si necesitas crear celdas auxiliares durante el desarrollo, puedes hacerlo pero debes asegurarte de borrarlas antes de entregar el notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218796d5-7901-47a5-9f24-15a010342200",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ea346213656fa3f7adfbabc399b5423",
     "grade": false,
     "grade_id": "cell-b39d9a6714c9effd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Sobre el dataset anterior (spotify_dataset.csv) se pide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aeadfd6-dcc5-4a16-ae27-cb77e418e8c2",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84b1bb462c457ed1bad0aefd634a9a75",
     "grade": false,
     "grade_id": "cell-41552ef3b7f22601",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 1 (1 punto)** Leerlo tratando de que Spark infiera el tipo de dato de cada columna, sin cachearlo.\n",
    "* Puesto que existen columnas que contienen una coma enmedio del valor, en esos casos los valores vienen entre comillas dobles. Spark ya contempla esta posibilidad y puede leerlas adecuadamente **si al leer le indicamos las siguientes opciones adicionales** además de las que ya sueles usar: `.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")`.\n",
    "* Asegúrate de que las **filas que no tienen el formato correcto sean descartadas**, indicando también la opción `mode` con el valor `DROPMALFORMED` como vimos en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a19274-5107-4f16-a4ee-f0fcb7108456",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a991784a8c6305d691f5b3f7cebe1ebd",
     "grade": false,
     "grade_id": "read_csv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n",
    "spotify_df = (\n",
    "             spark.read\n",
    "             .option('header', 'true')\n",
    "             .option('inferschema', 'true')\n",
    "             .option('quote','\\\"')\n",
    "             .option('escape','\\\"')\n",
    "             .option('mode','DROPMALFORMED')\n",
    "             .csv('abfss://datos-tarea@masterrop001sta.dfs.core.windows.net/spotify_dataset.csv')\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbaf78f-eb11-4a0c-8532-d5ee2b1f816f",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8675d8ca2c8833d9bb61bc265fd03a9b",
     "grade": true,
     "grade_id": "read_csv_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(spotify_df.count() == 277965)\n",
    "assert(spotify_df.is_cached == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9791c688-9f61-48fa-a275-5c5d57ca1337",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9d676ba27979472021841281b45b044",
     "grade": false,
     "grade_id": "cell-284e8bf4964733e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 2 (2 puntos)** Limpiar y enriquecer las variables aplicando las siguientes transformaciones encadenadas a `spotify_df`:\n",
    "* Reemplazar la columna `WeekID` por su versión convertida a tipo fecha (DateType). Esto se hará en una sola línea:\n",
    "  * Primero se convierte en una columna de Timestamp  con `F.from_unixtime(F.unix_timestamp('nombreColumna', 'dd-MM-yyyy')`\n",
    "  * En esa misma línea, dicha columna debe ser convertida a tipo DateType usando el método de la clase Column visto en clase.\n",
    "* Crear una nueva columna `title_length` con la longitud (número de caracteres) del título de cada canción. PISTA: consulta la [documentación oficial](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.length.html) de la función `F.length`.\n",
    "* Reemplazar la columna `Performer` por el resultado de *cortar cada título por el string* `\" & \"` (el carácter & precedido y seguido de un espacio en blanco), lo cual te devolverá una columna de vectores de string. PISTA: consulta la [documentación oficial](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.split.html) de la función `F.split`. En el caso de no haber ningún & el vector devuelto tendrá 1 elemento.\n",
    "* Crear una nueva columna `n_performers` con el número de intérpretes de una canción. Para ello, debes *contar el número de elementos* de cada vector de la columna `Performer` y [la documentación](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.size.html) de la función `F.size` (ojo, no confundir con `length` utilizada antes). Ambas se pueden concatenar ya que actúan sobre un objeto Column y devuelven otro objeto Column. No te preocupes por los elementos nulos que hubiese en esta columna.\n",
    "* Añadir dos columnas `year` y `month` que contengan respectivamente el año y el mes al que corresponde la fecha de la columna `WeekID`. PISTA: consulta la documentación oficial de [la función F.year](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.year.html) y de [la función F.month](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.month.html). \n",
    "* Guardar el resultado de las tres transformaciones anteriores en una nueva variable `spotify_enriquecido_df` y **cachearla**.\n",
    "\n",
    "* Guardar en la variable `fila_mas_autores` el objeto Row de la canción con el **mayor número de intérpretes**, deshaciendo los posibles empates en primer lugar por la que tenga la **menor longitud en el título** y, si sigue habiendo empates, por la que tenga la **semana más reciente** (cuidado: los tres criterios de ordenación son independientes y contrapuestos. Recuerda las funciones `asc()` y `desc()` de los objetos columna). PISTA: utilizar la acción `first()` tras ordenar el dataframe `spotify_enriquecido_df`, el cual no está ni debe quedar ordenado, puesto que la versión ordenada de dicho dataframe no debe reemplazar a dicha variable, sino que puedes usar otra variable auxiliar para guardarla y aplicar `first()` sobre ella.\n",
    "* Accediendo a los campos del objeto anterior, almacenar en la variable `titulo_mas_autores` y `url_mas_autores` los campos del título de la canción y su URL, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c26934e-c8ef-4c87-9261-58502c14939b",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebb59d02aa0d70c346d3564ecd3bf6e0",
     "grade": false,
     "grade_id": "enriquecer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "spotify_enriquecido_df = spotify_df.withColumn(\"WeekID\", F.from_unixtime(F.unix_timestamp(\"WeekID\", \"dd-MM-yyyy\")).cast(\"date\"))\\\n",
    "                                   .withColumn('title_length', F.length(F.col('Song')))\\\n",
    "                                   .withColumn('Performer', F.split(F.col('Performer'), ' & '))\\\n",
    "                                   .withColumn('n_performers', F.size(F.col('Performer')))\\\n",
    "                                   .withColumn('year', F.year(F.col('WeekID')))\\\n",
    "                                   .withColumn('month', F.month(F.col('WeekID'))).cache()\n",
    "\n",
    "fila_mas_autores = spotify_enriquecido_df.orderBy(F.col('n_performers').desc(),\n",
    "                                                  F.col('title_length').asc(),\n",
    "                                                  F.col('WeekID').desc())\\\n",
    "                                                  .first()\n",
    "                                                  \n",
    "\n",
    "titulo_mas_autores = fila_mas_autores['Song']\n",
    "url_mas_autores = fila_mas_autores['url']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8f5ee7-1a82-43d9-b53f-0140d8b0c0cc",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b50ce088825ea9f42e8c2bb0f474d7b0",
     "grade": true,
     "grade_id": "enriquecer_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tipos = dict(spotify_enriquecido_df.dtypes)\n",
    "assert(tipos[\"title_length\"] == \"int\" and \n",
    "       tipos[\"Performer\"] == \"array<string>\" and \n",
    "       tipos[\"n_performers\"] == \"int\")\n",
    "assert(sum([1 for x in [\"title_length\", \"n_performers\", \"year\", \"month\"] if x in spotify_enriquecido_df.columns]) == 4)\n",
    "assert(fila_mas_autores.n_performers == 3) \n",
    "assert(fila_mas_autores.url == \"http://www.billboard.com/charts/hot-100/1998-07-11\")\n",
    "r = spotify_enriquecido_df.select(F.mean(\"n_performers\").alias(\"n_performers\"),\n",
    "                                  F.mean(\"title_length\").alias(\"title_length\")).first()\n",
    "assert(round(r.n_performers, 2) == 1.07)\n",
    "assert(round(r.title_length, 2) == 15.59)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a1809f-5ee2-453f-8d6a-070da83de079",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a28dfeaf292d1cdca19dcd73dbbe9dee",
     "grade": false,
     "grade_id": "cell-58e421fdc7111d32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 3 (1.5 puntos)** Partiendo del DataFrame `spotify_enriquecido_df` se pide:\n",
    "\n",
    "* Crear un nuevo DataFrame que tenga tantas filas como años distintos aparecen, y tantas columnas como meses del año más una (que es el año y debe estar a la izquierda del todo). Las columnas deben aparecer de izqda a dcha del 1 a 12 -recuerda que el método `select()` te las devuelve en el orden que se las pidas-. En cada casilla de dicho DF debe aparecer el *número de canciones* ***distintas*** que han estado durante ese mes en algún momento en el Top 10. Se deben contar canciones distintas atendiendo a la columna `SongID`.\n",
    "  * PISTA: el punto de partida debe ser el DF formado solamente por aquellas semanas que están en el top10. A partir de ahí debes construir la tabla anterior. Recuerda qué función de `pyspark.sql.functions` te sirve para contar el número de elementos **distintos** de una columna o en este caso, de un grupo.\n",
    "  * PISTA: examina los valores posibles de la columna `top10_tag` para deducir con cuáles debes quedarte como punto de partida.\n",
    "* Rellenar los valores nulos del DataFrame resultante con 0.\n",
    "* Tras rellenarlos, añadir una nueva columna `n_top10_promedio` que contenga para cada año (fila) el número medio (redondeado a 2 cifras decimales con la función `F.round`) de canciones distintas que hay en todos los meses de ese año. \n",
    "  * PISTA: esta operación se realiza exclusivamente mediante operaciones aritméticas con columnas. No se necesita F.mean ni F.sum ni nada parecido. La función `F.round` se debe aplicar al objeto columna resultante de la operación aritmética.\n",
    "* Las filas del DataFrame resultante deben quedar ordenadas de menor a mayor año.\n",
    "* Guardar el resultado de las transformaciones anteriores en una nueva variable `distintos_top10_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eeba72f-d107-4663-b1f2-be6f9a6899c0",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0417d7385ba6c8b964d5c79513f92083",
     "grade": false,
     "grade_id": "anios_top10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "distintos_top10_preliminar = spotify_enriquecido_df.filter(F.col('top10_tag') == 'Top 10')\\\n",
    "                                           .groupBy(F.col('year'))\\\n",
    "                                           .pivot(\"month\", [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\"])\\\n",
    "                                           .agg(F.countDistinct(F.col('SongID')))\\\n",
    "                                           .fillna(0)\\\n",
    "                                           \n",
    "                                                    \n",
    "distintos_top10_df= distintos_top10_preliminar.withColumn(\"n_top10_promedio\", \n",
    "                                                          F.round(sum(F.col(c) for c in distintos_top10_preliminar.columns[1:])/12,2))\\\n",
    "                                              .orderBy(F.col('year').asc())                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50e2c39-fe32-40fe-9873-46579afa41cf",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64caf53cd93779a62d4fe863136b6394",
     "grade": true,
     "grade_id": "anios_top10_test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lista = distintos_top10_df.take(5)\n",
    "assert(lista[0].year == 1958 and lista[0][\"12\"] == 12 and lista[0].n_top10_promedio == 9.42)\n",
    "assert(lista[2].year == 1960 and lista[2][\"4\"] == 21 and lista[2].n_top10_promedio == 24.58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce809cb-cb65-4103-a166-365563ab0f16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Ejercicio 4 (1.5 puntos)** Partiendo de nuevo del DataFrame almacenado en `spotify_enriquecido_df` se pide:\n",
    "\n",
    "* Reemplazar la columna `top10_tag`, que actualmente son \"Top 10\" y \"Other\" (literalmente) por una versión recategorizada (usando `F.when`) cuyos valores sean los *números enteros* 1 y 0, respectivamente (es decir, Top 10 -> 1 y Other -> 0). No es necesario usar F.lit, aunque puedes hacerlo.\n",
    "* Crear una nueva columna `semanas_top10` que contenga para cada canción el número **total** de semanas que ha estado en el Top 10 **en ese mismo año** al que corresponde la fila. Está prohibido utilizar JOIN. Debe resolverse obligatoriamente creando una ventana en la  variable `w` (ATENCIÓN: hay que contar para **cada canción** y **ese mismo año**).\n",
    "  * PISTA: vas a necesitar la columna `top10_tag` que has recategorizado como enteros.\n",
    "  * PISTA: recuerda importar la función Window de `pyspark.sql`.\n",
    "* A continuación, seleccionar las columnas `SongID`, `Performer` y `semanas_top10` y quitar duplicados.\n",
    "* Ordenar el DataFrame resultante descentemente en base a `semanas_top10` y desempatar descendentemente en base a `SongID` (Spark utilizará orden alfabético al ser de tipo string).\n",
    "* Guardar el resultado de las transformaciones anteriores en la variable `semanas_top10_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ca180f-98ca-4ef3-872e-252e9cc6cb7b",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11e732ab7b3af13d17daea6ddc131e32",
     "grade": false,
     "grade_id": "pegar_top10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window().partitionBy('Song','year')\n",
    "semanas_top10_df = spotify_enriquecido_df.withColumn('top10_tag',\n",
    "                                           F.when(F.col('top10_tag') == 'Top 10',1).otherwise(0))\\\n",
    "                                          .withColumn('semanas_top10', F.sum(F.col('top10_tag')).over(w))\\\n",
    "                                          .dropDuplicates(['SongID', 'Performer', 'semanas_top10'])\\\n",
    "                                          .orderBy(F.col('semanas_top10').desc(),F.col('SongID').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c8f2e6-16d5-4454-b857-770fffb222a1",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c89515020e46b13a08c0a62dff0a6f9d",
     "grade": true,
     "grade_id": "pegar_top10_test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lista = semanas_top10_df.take(4)\n",
    "assert(lista[0].semanas_top10 == 30 and \"Bad Guy\" in lista[0].SongID)\n",
    "assert(lista[1].semanas_top10 == 28)\n",
    "assert(lista[2].semanas_top10 == 28)\n",
    "assert(lista[3].semanas_top10 == 27 and \"Girls Like You\" in lista[3].SongID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f00b9b-73c5-4317-873c-0144fd056ded",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab04e483657295cb6047f3a0a5e8af94",
     "grade": false,
     "grade_id": "cell-67b8088c6544f3a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 5 (1 punto)** Partiendo del DataFrame almacenado en `semanas_top10_df`:\n",
    "\n",
    "* Reemplazar la columna `Performer` por el resultado de *explotarla* utilizando la [función](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html) `F.explode(...)`, esto es,  desenrollar los vectores que hay en dicha columna, de manera que cada valor del vector pase a tener su propia fila completa. Por ejemplo si una casilla de esa columna contiene el vector \\[\"Beyonce\", \"Pitbull\"\\] porque hayan colaborado juntos en alguna canción, la operación `F.explode` sobre esa columna dará lugar a dos filas en el DataFrame resultante, que serán idénticas entre sí en todas las columnas excepto en la columna `Performer`, donde una de las dos filas tendrá el valor \"Beyonce\" y la otra fila tendrá el valor \"Pitbull\". El DataFrame resultante tendrá probablemente bastantes más filas que el original.\n",
    "* A continuación, encadenando una transformación aplicada al DF devuelto por el apartado anterior, crear un nuevo DataFrame con tantas filas como intérpretes distintos y dos columnas, que sean el nombre dintérprete y el número **total** de semanas que sus canciones han estado en el top 10, llamando a dicha columna `total_semanas_top10` (poniendo sobre la marcha en el momento de crearla).\n",
    "* Ordenar dicho DataFrame descendentemente en base a `total_semanas_top10`, y guardar el resultado en una variable `top_performers_df`\n",
    "* Utilizar una acción que lleve al Driver, a la variable `top_performers`, una lista de Python con el top 3 de estos artistas. ¿Te suenan...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49be81a6-eb71-4551-b2d3-0c19d5be9c48",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ece63cff5860a2fcd2fca717971eafa4",
     "grade": false,
     "grade_id": "mejores_artistas",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "top_performers_df = semanas_top10_df.withColumn('Performer', F.explode('Performer'))\\\n",
    "                                 .groupBy('Performer')\\\n",
    "                                 .agg(F.sum('semanas_top10').alias('total_semanas_top10'))\\\n",
    "                                 .orderBy(F.col('total_semanas_top10').desc())\n",
    "                                                          \n",
    "top_performers = top_performers_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8fea5e-f4c5-452c-aa4d-22a8756a8eae",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5d764c10da963b9f70a5c693e18f653",
     "grade": true,
     "grade_id": "mejores_artistas_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2968594256730435>, line 4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(top_performers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mPerformer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMariah Carey\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m top_performers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtotal_semanas_top10 \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m208\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(top_performers[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mPerformer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMadonna\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m top_performers[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtotal_semanas_top10 \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m205\u001B[39m)\n",
       "\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(top_performers[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mPerformer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe Beatles\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m top_performers[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mtotal_semanas_top10 \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m182\u001B[39m)\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AssertionError",
        "evalue": ""
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: "
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-2968594256730435>, line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(top_performers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mPerformer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMariah Carey\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m top_performers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtotal_semanas_top10 \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m208\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(top_performers[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mPerformer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMadonna\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m top_performers[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtotal_semanas_top10 \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m205\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(top_performers[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mPerformer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe Beatles\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m top_performers[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mtotal_semanas_top10 \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m182\u001B[39m)\n",
        "\u001B[0;31mAssertionError\u001B[0m: "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert(top_performers_df.count() == 7992)\n",
    "assert(top_performers[0].Performer == \"Mariah Carey\" and top_performers[0].total_semanas_top10 == 208)\n",
    "assert(top_performers[1].Performer == \"Madonna\" and top_performers[1].total_semanas_top10 == 205)\n",
    "assert(top_performers[2].Performer == \"The Beatles\" and top_performers[2].total_semanas_top10 == 182)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84499294-40a6-42d1-9c19-20b10a3306f9",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b766faeca84b2f8e45fc02fc2b19ce3",
     "grade": false,
     "grade_id": "intro_clustering",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Este ejercicio y los dos siguientes plantean un algoritmo de **clustering (K-means)** sobre las canciones, basándonos solamente en los atributos numéricos que describen los parámetros de cada canción. Concretamente, usaremos solamente las siguientes 10 columnas:\n",
    "\n",
    "* danceability: métrica que define la probabilidad de que la canción se pueda bailar\n",
    "* energy: Métrica que define qué tan enérgica es la canción\n",
    "* loudness: métrica que define el volumen de la canción\n",
    "* mode: pregrabación estéreo o mono (tiene dos valores: 0 y 1)\n",
    "* speechiness: Métrica que define cuánto canto vs instrumento hay en la canción\n",
    "* acousticness: métrica que define el grado en que se utilizan los instrumentos acústicos.\n",
    "* instrumentalness: métrica que define cuánto de la canción utiliza instrumentos\n",
    "* liveness: métrica que define el estado de ánimo de una canción\n",
    "* valence: Métrica que define la positividad de la canción\n",
    "* tempo: Métrica que define el tempo de la canción\n",
    "\n",
    "Puesto que K-means con distancia euclídea requiere que todas las variables estén aproximadamente en el mismo rango para que todas influyan por igual en la distancia, primero vamos a reescalar todas las variables anteriores mediante un MinMaxScaler de Spark. Eso significa que tras el reescalado, todas tendrán valores entre 0 y 1. Como el MinMaxScaler requiere una columna de tipo vector como entrada porque es capaz de reescalar de una sola vez muchas features, construiremos **antes del reescalado** una columna de tipo vector con las features originales. La salida del reescalado sigue siendo una columna de tipo vector, tal como exige el algoritmo K-means que entrenaremos después."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0d1bcd-2520-46fa-82d2-e571ab1be7ed",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "750c6adc044669e0f77d9ec25f90a3b3",
     "grade": false,
     "grade_id": "cell-9065e931deeb24cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 6 (1 punto)** Partiendo del DataFrame `spotify_enriquecido_df`, tal como fue leído del fichero CSV, se pide:\n",
    "\n",
    "* Ejecutar el código dado para crear el DataFrame `spotify_numeric_df`. No debes resolver nada en este momento, sino dejar este código sin modificar.\n",
    "* Crear en la variable `vector_assembler` un VectorAssembler que reciba como entradas (argumento `inputCols`) la lista `cluster_vars` que ya viene creada, y como salida cree una nueva columna llamada `feats_vectorizadas`.\n",
    "* Crear en la variable `minmax_scaler` un [objeto `MinMaxScaler()` de Spark](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinMaxScaler.html) que esté configurado para utilizar como entrada la columna `feats_vectorizadas`, y que cree una nueva columna de tipo vector llamada `feats_reescaladas`.\n",
    "  * **NOTA IMPORTANTE**:  `MinMaxScaler` recibe como única columna de entrada una columna de tipo vector, ya que es capaz de re-escalar de una sola vez (y de manera independiente) un conjunto de columnas (o mejor dicho, una columna de tipo vector que en realidad representa varias columnas independientes ya colapsadas). Por eso en este caso particular, la penúltima pieza del pipeline, que va justo antes del algoritmo K-Means, **NO** va a ser el `vector_assembler` como suele ser habitual, sino el `minmax_scaler`, y por eso el `vector_assembler` debe ir antes de dicha pieza, ya que es quien va a crear la columna de vectores que después recibirá el `minmax_scaler` como entrada.\n",
    "  * NOTA: los parámetros de valor mínimo y máximo que debemos indicar al MinMaxScaler como extremos de la columna que va a crear nueva, vienen ya fijados por defecto a 0 y 1 respectivamente (es decir, cada columna se va a re-escalar, dando lugar a otra nueva que va entre 0 y 1). Esto es justo lo que queremos para K-means, así que los dejamos sin especificar para que tomen esos valores.\n",
    "* Crear en la variable `kmeans`un [estimador K-Means](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html) con argumento `k=10` clusters, que reciba como entrada la columna `feats_reescaladas` creada en la etapa del vector assembler, y genere como salida (argumento `predictionCol`) una nueva columna llamada `cluster`, que haga como máximo 200 iteraciones (argumento `maxIter`) y que tenga semilla 12345 (argumento `seed`, imprescindible para que coincidan los resultados de los tests).\n",
    "  * A priori desconocemos el número de clusters que necesitamos, que no tiene por qué coincidir con el número de géneros musicales ya que también estamos juntando datos de muchos años diferentes y esto también influye. Deberíamos hacer un análisis con regla del codo y tratar de interpretar los clusters para varios valores de k distintos hasta quedar contentos con un valor de k, pero para simplificar, omitiremos este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15608c65-aa52-4644-bea2-6cd909e460ae",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63807dcffc68d658c598bbf100f977c1",
     "grade": false,
     "grade_id": "piezas",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler   # recuerda los imports necesarios para MinMaxScaler y VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans           # recuerda el import necesario para el algoritmo KMeans\n",
    "\n",
    "cluster_vars = [\"danceability\", \"energy\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \n",
    "                \"liveness\", \"valence\", \"tempo\"]\n",
    "\n",
    "spotify_numeric_df = spotify_enriquecido_df\\\n",
    "    .groupBy(\"SongID\").agg(*[F.first(c).alias(c) for c in [\"year\", \"spotify_track_duration_ms\"] + cluster_vars])\\\n",
    "    .distinct()\\\n",
    "    .cache()\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\\\n",
    "                            .setInputCol('feats_vectorizadas')\\\n",
    "                            .setOutputCol('feats_reescaladas')\n",
    "\n",
    "vector_assembler =  VectorAssembler()\\\n",
    "                    .setInputCols(cluster_vars)\\\n",
    "                    .setOutputCol('feats_vectorizadas')\\\n",
    "                                        \n",
    "kmeans = KMeans(\n",
    "                k=10,\n",
    "                maxIter=200,\n",
    "                seed=12345,\n",
    "                featuresCol=\"feats_reescaladas\",\n",
    "                predictionCol=\"cluster\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961df301-e51f-45b8-91ef-14b0fe759529",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8786c082c39ebf4b99d50d4cc2bd0b0f",
     "grade": true,
     "grade_id": "piezas_test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(spotify_numeric_df.count() == 23751)\n",
    "assert(len(spotify_numeric_df.columns) == 13)\n",
    "assert(minmax_scaler.getInputCol() == \"feats_vectorizadas\")\n",
    "assert(minmax_scaler.getOutputCol() == \"feats_reescaladas\")\n",
    "assert(vector_assembler.getInputCols() == cluster_vars)\n",
    "assert(kmeans.getK() == 10 and kmeans.getFeaturesCol() == \"feats_reescaladas\" \n",
    "       and kmeans.getPredictionCol() == \"cluster\" and kmeans.getMaxIter() == 200\n",
    "       and kmeans.getSeed() == 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fe3ed3-a286-4825-bdd6-ab47948c8005",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6ba1db0081758452a16b1a0e84ac109",
     "grade": false,
     "grade_id": "cell-7938478156b24ff2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 7 (2 puntos)** \n",
    "* Crear un pipeline en la variable `pipeline` que tenga las tres etapas anteriores (el vector_assembler, el minmax_scaler y el kmeans) por ese orden.\n",
    "* Entrenar el pipeline sobre el DF `spotify_numeric_df` y guardar el resultado (pipeline entrenado) en la variable `pipeline_model`.\n",
    "* Aplicar el pipeline entrenado para transformar el DataFrame `spotify_numeric_df` y guardar el resultado en la variable `clustered_songs_df`, que debe quedarse **cacheada**.\n",
    "* Analizar el resultado obtenido mostrando métricas agregadas de las variables dentro de cada cluster. Para ello, a partir de `clustered_songs_df`, crear en la variable `metricas_clusters_pd` un nuevo DataFrame con tantas filas como clusters (10) y que tenga 13 columnas que serán:\n",
    "  * La columna más a la izquierda el cluster al que corresponde la fila, que es por lo que estamos agrupando.\n",
    "  * La segunda será la mediana de la columna `year` en cada grupo. Dicha columna no se utilizó para el clustering pero existe en el DF `clustered_songs_df` \n",
    "  * Las siguientes 10 columnas deben ser la mediana de cada una de las 10 variables numéricas utilizadas para el cluster (excepto para \"mode\" que debemos usar la media y no la mediana)\n",
    "  * La última columna debe ser el recuento del número de filas de cada grupo, que debe renombrarse como `n`. \n",
    "* Tras el cálculo, el DataFrame resultante debe pasarse a DataFrame de **pandas**, y eso es lo que debe guardarse en `metricas_clusters_pd`. \n",
    "  * Se debe usar la **mediana** de cada columna en cada grupo. Puesto que la mediana no está disponible en la API de columnas en Spark 2.4 (se incluyó en versiones posteriores) pero sí que está disponible en la API de SQL puro, tenemos que utilizar para cada agregación la expresión `F.expr(\"percentile_approx(nombreColumna, 0.5)\").alias(nombreColumna)` que devuelve un objeto Column con la agregación y ya renombrado para llamarse igual que la columna a la que le estamos aplicando la mediana. \n",
    "  * En la columna `mode` se debe usar la **media** habitual, no la mediana, también renombrada para que se llame \"mode\" tras aplicar la agregación.\n",
    "  * NOTA: estamos haciendo clustering, con lo que no hay ninguna división en train y test ni nada parecido! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924ca930-9a63-4801-81e2-8a78b679199a",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5d7a7dfb0fb8cf58a379e7d3032a13e",
     "grade": false,
     "grade_id": "salidas_clusters",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline             # no olvides el import necesario para el Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[vector_assembler, minmax_scaler, kmeans])\n",
    "pipeline_model = pipeline.fit(spotify_numeric_df)\n",
    "clustered_songs_df = pipeline_model.transform(spotify_numeric_df).cache()\n",
    "\n",
    "metricas_clusters_pd = clustered_songs_df.groupBy('cluster')\\\n",
    "                        .agg(F.median('year').alias('year'),\n",
    "                            *[F.expr(f'percentile_approx({c}, 0.5)').alias(c) if c != 'mode' else F.mean(c).alias(c) for c in clustered_songs_df.columns[3:13]],\n",
    "                            F.count('*').alias('n')\n",
    "                            )\\\n",
    "                        .toPandas()\n",
    "\n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf07d65-9b5e-4461-b240-743a5c8af01a",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9299cdb0856ecd55483bd16eeeadf2a3",
     "grade": true,
     "grade_id": "salidas_clusters_test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(metricas_clusters_pd.shape == (10, 13))\n",
    "columnas = [\"cluster\", \"year\", \"n\"] + cluster_vars   # todas estas columnas tienen que existir en metricas_clusters_pd\n",
    "assert(sum([1 for x in columnas if x in metricas_clusters_pd.columns]) == 13)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Tarea Spark - Roberto Oliva Pastor",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
